
## UNIT 1: SUPERVISED LEARNING PROMPTS

### Basic Methods
"Create comprehensive study notes on basic machine learning methods covering: (1) Distance-based methods with metrics (Euclidean, Manhattan, Minkowski) and their properties, (2) k-Nearest Neighbors algorithm with implementation steps, choosing k value, weighted kNN, and handling curse of dimensionality, (3) Decision Trees including ID3, C4.5, CART algorithms with splitting criteria (Gini, Entropy, Information Gain), pruning techniques, and handling continuous attributes, (4) Naive Bayes classifier with probability theory foundation, Laplace smoothing, text classification example, and assumptions analysis."

### Linear Models
"Generate detailed notes on linear models including: (1) Linear Regression with ordinary least squares, gradient descent optimization, polynomial regression, regularization (Ridge, Lasso, Elastic Net), and evaluation metrics (MSE, RMSE, R²), (2) Logistic Regression with sigmoid function, maximum likelihood estimation, multi-class extensions (one-vs-all, softmax), and interpretation of coefficients, (3) Generalized Linear Models (GLM) framework covering link functions, exponential family distributions, and applications for different data types."

### Support Vector Machines
"Develop comprehensive notes on SVM covering: (1) Linear SVM formulation with margin maximization, hard and soft margin concepts, (2) Dual formulation and support vectors identification, (3) Kernel trick explanation with polynomial, RBF, and sigmoid kernels, (4) Kernel selection and hyperparameter tuning strategies, (5) SVM for regression (SVR) and practical implementation tips."

### Beyond Binary Classification
"Create notes on advanced classification topics including: (1) Multi-class classification strategies (One-vs-One, One-vs-All, ECOC) with pros and cons, (2) Structured output prediction with examples from NLP and computer vision, (3) Learning to rank algorithms including pointwise, pairwise, and listwise approaches with applications in information retrieval."

## UNIT 2: UNSUPERVISED LEARNING PROMPTS

### Clustering
"Generate detailed notes on clustering covering: (1) K-means algorithm with initialization methods (random, k-means++), convergence criteria, and choosing optimal k (elbow method, silhouette score), (2) Kernel K-means for non-linear clustering with kernel selection, (3) Hierarchical clustering (agglomerative and divisive) with linkage criteria, (4) DBSCAN and Gaussian Mixture Models as alternatives."

### Dimensionality Reduction
"Create comprehensive notes on dimensionality reduction including: (1) Principal Component Analysis (PCA) with mathematical foundation, eigenvalue decomposition, variance explained, and scree plots, (2) Kernel PCA for non-linear dimensionality reduction with examples, (3) Comparison with other methods (LDA, t-SNE, UMAP) and selection criteria."

### Matrix Operations
"Develop notes on matrix techniques covering: (1) Matrix factorization methods (SVD, NMF) with applications in recommendation systems, (2) Matrix completion for handling missing data with nuclear norm minimization, (3) Collaborative filtering and latent factor models with real-world examples."

### Generative Models
"Generate notes on generative models including: (1) Gaussian Mixture Models (GMM) with EM algorithm derivation and implementation, (2) Latent Dirichlet Allocation (LDA) for topic modeling, (3) Comparison between discriminative and generative approaches with use cases."

## UNIT 3: MODEL EVALUATION AND ENSEMBLE METHODS PROMPTS

### Model Evaluation
"Create comprehensive notes on model evaluation covering: (1) Cross-validation techniques (k-fold, stratified, leave-one-out) with implementation, (2) Evaluation metrics for classification (accuracy, precision, recall, F1, AUC-ROC) and regression (MAE, MSE, R²), (3) Confusion matrix interpretation and multi-class metrics, (4) Bias-variance tradeoff with decomposition and visualization, (5) Model selection strategies and hyperparameter tuning (grid search, random search, Bayesian optimization)."

### Statistical Learning Theory
"Develop notes on learning theory including: (1) PAC learning framework and sample complexity, (2) VC dimension and its implications, (3) Rademacher complexity and generalization bounds, (4) Regularization from theoretical perspective."

### Ensemble Methods
"Generate detailed notes on ensemble methods covering: (1) Bagging with bootstrap sampling, out-of-bag error, and variance reduction analysis, (2) Random Forests with feature importance, hyperparameter tuning, and advantages, (3) Boosting algorithms (AdaBoost, Gradient Boosting, XGBoost) with mathematical formulation and implementation, (4) Stacking and blending techniques for model combination."

## UNIT 4: ADVANCED TOPICS PROMPTS

### Sparse Modeling
"Create notes on sparse modeling including: (1) L1 regularization and sparsity-inducing norms, (2) LASSO and its variants (Group LASSO, Fused LASSO), (3) Sparse coding and dictionary learning, (4) Applications in feature selection and compressed sensing."

### Sequential Data
"Develop comprehensive notes on sequence modeling covering: (1) Time series analysis fundamentals (stationarity, autocorrelation), (2) ARIMA models and forecasting techniques, (3) Hidden Markov Models with forward-backward algorithm, (4) Recurrent Neural Networks (vanilla RNN, LSTM, GRU) with applications."

### Deep Learning
"Generate detailed notes on deep learning including: (1) Neural network fundamentals with backpropagation derivation, (2) Activation functions comparison and selection criteria, (3) Convolutional Neural Networks (CNN) architecture and applications, (4) Autoencoders for feature learning and dimensionality reduction, (5) Transfer learning and fine-tuning strategies."

## UNIT 5: SCALABLE AND MODERN LEARNING PROMPTS

### Scalable Machine Learning
"Create comprehensive notes on scalable ML covering: (1) Online learning algorithms (SGD, online gradient descent) with regret analysis, (2) Mini-batch processing and learning rate schedules, (3) Distributed machine learning with MapReduce and parameter servers, (4) Federated learning concepts and privacy considerations."

### Semi-supervised Learning
"Develop notes on semi-supervised learning including: (1) Self-training and co-training algorithms, (2) Graph-based methods with label propagation, (3) Generative models for semi-supervised learning, (4) When and why semi-supervised learning works."

### Active Learning
"Generate notes on active learning covering: (1) Query strategies (uncertainty sampling, QBC, expected error reduction), (2) Pool-based vs stream-based active learning, (3) Practical implementation and stopping criteria, (4) Applications in labeling cost reduction."

### Reinforcement Learning
"Create introductory notes on RL including: (1) Markov Decision Processes and Bellman equations, (2) Value-based methods (Q-learning, SARSA), (3) Policy gradient methods basics, (4) Exploration vs exploitation tradeoff."

### Graphical Models
"Develop notes on graphical models covering: (1) Bayesian networks representation and independence, (2) Markov Random Fields and factor graphs, (3) Inference algorithms (variable elimination, belief propagation), (4) Learning structure and parameters."

### Bayesian Learning
"Generate comprehensive notes on Bayesian methods including: (1) Bayes theorem and prior/posterior concepts, (2) Bayesian linear regression with uncertainty quantification, (3) Gaussian processes for non-parametric learning, (4) MCMC methods for approximate inference."

## UNIT 6: RECENT TRENDS PROMPTS

### Current Developments
"Create notes on recent ML trends covering: (1) Transformer architectures and attention mechanisms, (2) Few-shot and zero-shot learning approaches, (3) Explainable AI and interpretability methods, (4) Adversarial machine learning and robustness, (5) AutoML and neural architecture search, (6) Graph neural networks and applications."

## INTEGRATION AND REVISION PROMPTS

### Mathematical Foundations
"Compile essential mathematics including: (1) Linear algebra operations for ML, (2) Probability distributions and properties, (3) Optimization techniques and convergence, (4) Key derivatives and gradients used in ML."

### Algorithm Comparison
"Create comprehensive comparison tables for: (1) Supervised learning algorithms with complexity, assumptions, and use cases, (2) Clustering algorithms with strengths and limitations, (3) Dimensionality reduction techniques, (4) Ensemble methods performance characteristics."

### Implementation Guide
"Develop practical implementation notes including: (1) Python/scikit-learn code snippets for each algorithm, (2) Common pitfalls and debugging strategies, (3) Data preprocessing best practices, (4) Model deployment considerations."

### Quick Reference
"Generate a quick reference guide with: (1) Algorithm selection flowchart, (2) Hyperparameter tuning guidelines, (3) Evaluation metric selection criteria, (4) Common interview questions with solutions, (5) Cheat sheet for time complexity of algorithms."
